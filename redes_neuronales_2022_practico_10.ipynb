{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoMichati/Redes-neuronales/blob/main/redes_neuronales_2022_practico_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red neuronal\n",
        "\n",
        "Típicamente, una red neuronal artificial consiste en un conjunto de $N$ unidades de procesamiento o \"neuronas\" que se interconectan entre si vía una matríz de interacciones \"sinápticas\" $w\\in \\mathbb{R}^{n\\times n}$.\n",
        "Más precisamente, el estado $x_i$ de la $i$-esima neurona se actualiza según\n",
        "\n",
        "$$\n",
        "x_i(t+1) = g\\bigg(\\sum_j w_{ij}x_j(t) - u_i\\bigg) \\;\\;\\;\\;\\;\\;\\;\\;(1)\n",
        "$$\n",
        "\n",
        "Aquí, la función $g:\\mathbb{R}\\to \\mathbb{R}$ es la misma para toda neurona, y se evalúa en términos de una suma pesada sobre el estado de todas las otras neuronas al tiempo anterior $t$, más un término umbral $u_i\\in \\mathbb{R}$,\n",
        "\n",
        "$$\n",
        "h_i(t) = \\sum_j w_{ij}x_j(t) - u_i\n",
        "$$\n",
        "\n",
        "que se denomina campo local asociado a la neurona $i$.\n",
        "La entrada $w_{ij}$ de la matriz $w$ corresponde a la conección sináptica que va desde la neurona $j$ hacia la neurona $i$; es decir, media el flujo de información desde $j$ hacia $i$.\n",
        "\n",
        "La actualización del estado de las neuronas puede ser sincrónico (todas actualizan su estado simultaneamente) o asincrónico (las neuronas actualizan su esado de a una a la vez y en orden típicamente aleatorio).\n",
        "\n",
        "El conjunto de neuronas suele dividirse en 3 subconjuntos: el de entrada $E$, el de salida $S$ y el oculto $O$.\n",
        "En este caso, el estado de las neuronas de entrada no se determina por la ecuación $(1)$, sinó que externamente a travéz de un *input*.\n",
        "La red se entrena utilizando un conjunto de ejemplos, cada uno constituido por un *input* y *output* correspondiente.\n",
        "Sea $e_{ki}$ el valor de la $i$-esima entrada del $k$-ésimo y $s_{ki}$ el valor de la $i$-ésima salida en el $k$-ésimo output.\n",
        "De esta manera, el $k$-ésimo ejemplo viene dado por el par $(e_k,s_k)$ de vectores $e_k$ y $s_k$.\n",
        "\n",
        "Para que aprenda del $k$-ésimo ejemplo, se compara el output $s_k$ del $k$-ésimo ejemplo, contra el output $x(e_k)$ generador por la red ante el $k$-ésimo input $e_k$.\n",
        "Luego, se reajustan los pesos sinápticos $w$ y los umbrales $u$, con la esperanza de reducir las \"diferencias\" entre el output esperado y el output generado.\n",
        "Este proceso suele repetirse, varias veces sobre cada ejemplo (elegidos secuencialmente de manera aleatoria), hasta que las \"diferencias\" entre outputs esperados y outputs generados se reducen a un valor aceptable según algún criterio a definir.\n",
        "\n",
        "## Red feed-forward\n",
        "\n",
        "En una red feed forward, las neuronas se organizan en capas.\n",
        "Las neuronas de entrada constituyen la primera capa, que en realidad no se suele contar, o se la denomina capa $0$, ya que las mismas se pueden representar por meras variables.\n",
        "Luego, pueden seguir varias capas de neuronas ocultas, y finalmente una capa de neuronas de salida que siempre existe.\n",
        "En este tipo de redes neuronales, las neuronas de una capa sólo reciben conecciones sinápticas de capas anteriores, y típicamente desde la capa anterior.\n",
        "Por ende, la matriz de conecciones sinápticas $w$ suele remplazarse por un tensor de grado 3, cuya entrada $w_{lij}$ corresponde a la conección sináptica que va desde la neurona $j$ en la capa $l$ hacia la neurona $i$ en la capa $l+1$.\n",
        "\n",
        "Como ejemplo, a continuación se esquematiza una red neuronal de dos capas, una oculta y una de salida. Posee 3 neuronas de entradas, 4 neuronas en la capa oculta y 2 neuronas en la capa de salida.\n",
        "Identificaremos el estado de la $i$-ésima neurona en la $l$-ésima capa con la variable $x_{li}$.\n",
        "Además, notar que con el fin de remplazar los umbrales $u_{li}$ por correspondientes conecciones sinápticas $w_{li0}$, se agregan \"neuronas fijas\" (las de color rosado e indexadas por $i=0$) que, por definición, permanecen siempre en el estado $x_{l0}=-1$.\n",
        "\n",
        "<img src=\"https://github.com/jipphysics/redes-neuronales-2022/blob/master/practicos/assets/red-feedforward.png?raw=true:, width=100\" alt=\"My Image\" width=300>\n",
        "\n",
        "## Perceptrón simple\n",
        "\n",
        "El perceptrón simple es una red feed-forward de una sola capa, i.e. no posee capas ocultas.\n",
        "\n",
        "### Referencias\n",
        "\n",
        "* Introduction to the theory of neural computations, J. Hertz, H. Krogh, R.G. Palmer, CRC Press (1991)\n",
        "\n",
        "## **Ejercicio 1)** Importando librerías\n",
        "\n",
        "Importe las librerías `numpy` para operar con arrays, `scipy` para utilizar rutinas de algebra lineal y `matplotlib.pyplot` para graficar."
      ],
      "metadata": {
        "id": "NRYEofSD0xoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.linalg as linalg\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "I8N3D_nU1_oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 2)** \n",
        "\n",
        "Consideremos el caso de un perceptrón simple con una única neurona de salida y la función signo como función activación (sección 5.2 del Hertz)\n",
        "\n",
        "$$\n",
        "g(h)\n",
        "=\n",
        "\\left\\{\n",
        "\\begin{array}{ll}\n",
        "1 & h > 0 \\\\\n",
        "-1 & c.c.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "**a)** Importe la función `make_classification` de scikit-learn para generar un conjunto de ejemplos a aprender. Para ello, escriba\n",
        "\n",
        "    from sklearn.datasets import make_classification\n",
        "\n",
        "**b)** Genere un conjunto de 10 ejemplos (`n_samples=10`). Los inputs deben ser vectores con dos coordenadas (`n_features=2`). Los outputs deben poseer dos valores (`n_classes=2`). Notar que `make_classification` genera salidas con valores en $\\{0,1\\}$, use la función $v\\to 2v-1$ para transformarlos a valores en $\\{-1,1\\}$. \n",
        "\n",
        "**c)** Grafique los ejemplos en el plano.\n",
        "\n",
        "**d)** Implemente un perceptrón simple de 2 entradas + 1 neurona fija, y una salida. Entrene la red utilizando la regla (ver ec. 5.23 del Hertz)\n",
        "\n",
        "$$\n",
        "\\Delta w_{i} = \\eta \\Theta\\bigg(\\epsilon-s_kh_k\\bigg)s_ke_{ki}\n",
        "$$\n",
        "\n",
        "iterando sequencialmente sobre los ejemlos de entrenamiento.\n",
        "Aquí, $s_k$ es la única componente del output del $k$-ésimo ejemplo, $e_{ki}$ es la $i$-ésima componente de la entrada del $k$-ésimo ejemplo,\n",
        "\n",
        "$$\n",
        "h_k\n",
        "=\n",
        "\\sum_{i=0}^N w_ie_{ki}\n",
        "$$\n",
        "\n",
        "es el campo local asociado a la única neurona de salida cuando la red es evaluada en el input del $k$-ésimo ejemplo.\n",
        "Además, $\\eta>0$ y $\\epsilon>0$ son parámetros pequeños que controlan la convergencia del proceso de entrenamiento y\n",
        "\n",
        "$$\n",
        "\\Theta(x)\n",
        "=\n",
        "\\left\\{\n",
        "\\begin{array}{ll}\n",
        "1 & x>0\\\\\n",
        "0 & c.c.\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "es la función escalón de Heaviside.\n",
        "\n",
        "**e)** Repitiendo **c)**, grafique la separatriz definida por $x \\to -w_2/w_1x+w_0$."
      ],
      "metadata": {
        "id": "NcaGEHAd10sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.a)\n",
        "from sklearn.datasets import make_classification"
      ],
      "metadata": {
        "id": "NUoQ9bnwaZ7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicio 3)** \n",
        "\n",
        "**a)** Realice la animación que les comentó Pancho y gánese un champan.\n",
        "\n",
        "**b)** Compártalo con los compañeros."
      ],
      "metadata": {
        "id": "kSYWEH6KsOxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.a)"
      ],
      "metadata": {
        "id": "GoBFYeswsYgk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}